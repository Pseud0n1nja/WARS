{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are _Best Film Recommenders (BFR)_, a company located in NYC and founded by two Columbia Students in 2014. Since our inception, we have been recognized by our users for our accurate and personalized movie recommendations. However, our industry is always evolving, requiring constant improvements to remain competitive and profitable.\n",
    "\n",
    "A week ago we received a letter from _Wes Anderson Productions (WAP)_, the company which owns the rights for every movie directed by Wes Anderson. Within the letter they claim that despite the high quality of Wes Anderson's films, most of his work is still unknown by most consumers. The only exceptions are _The Grand Budapest Hotel (2014)_ (which received critical acclaim, including nine Oscars nominations), and _Isle of Dogs (2018)_, which popularity was boosted by the former's success.\n",
    "\n",
    "Due to WAP films having high ratings yet low viewership, the company believes that six of Wes Anderson's films are unpopular due to a lack of publicity when they were released. With the Christmas season approaching, WAP believe that target advertising can bring in new viewers that never knew the movies existed, increasing revenue from movie sales. For this reason, WAP are willing to pay a significant sum of money if we can reach 1,000 users, representing a significant potential income for our company.\n",
    "\n",
    "While we could recommend the six movies to all our users, our company's mission has always been focussed on our users, and we don't want to make recommendations they can find irrelevant. Because of this, we would like to find the movie which is most likely to be enjoyed by each user among the following films: \n",
    "\n",
    "* _Rushmore (1998)_\n",
    "* _The Royal Tenenbaums (2001)_\n",
    "* _The Life Aquatic with Steve Zissou (2004)_\n",
    "* _The Darjeeling Limited (2007)_\n",
    "* _Fantastic Mr. Fox (2009)_\n",
    "* _Moonrise Kingdom (2012)_\n",
    "\n",
    "Therefore, our goal will be to build a new _Wes Anderson Recommendation System (WARS)_, to find the ideal movie per user. While we have no _accuracy cutoff_, we want our recommendations to be as targeted as possible to keep a high availability of advertising space, should new clients wish to use our services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this initial phase our goal is to play with the data so that we have a clearer idea of which route to take in building a robust final product.\n",
    "\n",
    "To maximize the breadth of our training, we will design datasets with different characteristics, try different kinds of recommendation techniques, and fit the models by shifting several hyperparameters. All with the intention to find what works and what doesn't for the first release of _WARS_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our previous recommendation systems, we are going to use subsets of the full data set for education and development from [MovieLens](https://grouplens.org/datasets/movielens/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv('movie_data/ratings.csv')\n",
    "df_ratings = df_ratings[df_ratings.userId.isin(range(9000, 10010))]\n",
    "df_movies = pd.read_csv('movie_data/movies.csv')\n",
    "df_tags = pd.read_csv('movie_data/tags.csv')\n",
    "df_genome_scores = pd.read_csv('movie_data/genome-scores.csv')\n",
    "df_genome_tags = pd.read_csv('movie_data/genome-tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>tag</th>\n",
       "      <th>timestamp_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>9005</td>\n",
       "      <td>2951</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1463826257</td>\n",
       "      <td>Fistful of Dollars, A (Per un pugno di dollari...</td>\n",
       "      <td>Action|Western</td>\n",
       "      <td>Clint Eastwood</td>\n",
       "      <td>1.463937e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>9005</td>\n",
       "      <td>2951</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1463826257</td>\n",
       "      <td>Fistful of Dollars, A (Per un pugno di dollari...</td>\n",
       "      <td>Action|Western</td>\n",
       "      <td>Ennio Morricone</td>\n",
       "      <td>1.463937e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>9005</td>\n",
       "      <td>2951</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1463826257</td>\n",
       "      <td>Fistful of Dollars, A (Per un pugno di dollari...</td>\n",
       "      <td>Action|Western</td>\n",
       "      <td>morricone score</td>\n",
       "      <td>1.463937e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     userId  movieId  rating   timestamp  \\\n",
       "464    9005     2951     4.5  1463826257   \n",
       "465    9005     2951     4.5  1463826257   \n",
       "466    9005     2951     4.5  1463826257   \n",
       "\n",
       "                                                 title          genres  \\\n",
       "464  Fistful of Dollars, A (Per un pugno di dollari...  Action|Western   \n",
       "465  Fistful of Dollars, A (Per un pugno di dollari...  Action|Western   \n",
       "466  Fistful of Dollars, A (Per un pugno di dollari...  Action|Western   \n",
       "\n",
       "                 tag  timestamp_tag  \n",
       "464   Clint Eastwood   1.463937e+09  \n",
       "465  Ennio Morricone   1.463937e+09  \n",
       "466  morricone score   1.463937e+09  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rm = pd.merge(df_ratings, df_movies, on='movieId', how='left')\n",
    "df_tags.rename(columns={'timestamp':'timestamp_tag'}, inplace=True)\n",
    "df_rmt = pd.merge(df_rm, df_tags, on=['userId','movieId'], how='left')\n",
    "df_rmt.dropna().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmt_mult_rows = df_rmt[['userId', 'movieId', 'rating', 'timestamp', 'genres', 'tag']]\n",
    "df_rmt_sing_row = df_abt_mult_rows\\\n",
    "    .groupby(['userId', 'movieId', 'rating', 'timestamp', 'genres'])\\\n",
    "    .agg({'tag': list})\\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>genres</th>\n",
       "      <th>tag</th>\n",
       "      <th>genome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9000</td>\n",
       "      <td>318</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1167308733</td>\n",
       "      <td>Crime|Drama</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[007, 0.02250000000000002], [007 (series), 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9000</td>\n",
       "      <td>337</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1167308940</td>\n",
       "      <td>Drama</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[007, 0.01924999999999999], [007 (series), 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9000</td>\n",
       "      <td>508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1167308922</td>\n",
       "      <td>Drama</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[007, 0.024749999999999998], [007 (series), 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp       genres    tag  \\\n",
       "0    9000      318     1.0  1167308733  Crime|Drama  [nan]   \n",
       "1    9000      337     1.0  1167308940        Drama  [nan]   \n",
       "2    9000      508     1.0  1167308922        Drama  [nan]   \n",
       "\n",
       "                                              genome  \n",
       "0  [[007, 0.02250000000000002], [007 (series), 0....  \n",
       "1  [[007, 0.01924999999999999], [007 (series), 0....  \n",
       "2  [[007, 0.024749999999999998], [007 (series), 0...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_genome = pd.merge(df_genome_scores, df_genome_tags, on=['tagId'], how='inner')\n",
    "df_movie_genome = df_genome\\\n",
    "    .groupby('movieId')[['tag', 'relevance']]\\\n",
    "    .apply(lambda x:pd.Series({'genome': x.values.tolist()}))\\\n",
    "    .reset_index()\n",
    "df_rmtg = pd.merge(df_rmt_sing_row, df_movie_genome, on=['movieId'], how='left')\n",
    "df_rmtg.dropna().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_values = df_movies['genres'].map(lambda x: x.split('|')).values\n",
    "genres = list(set([genre for movie_genres in genres_values for genre in movie_genres if genre != \"(no genres listed)\"]))\n",
    "genres_dict = {genres[i]: i for i in range(0,len(genres))}\n",
    "\n",
    "def get_genres_features(genres_str, genres_dict):\n",
    "    genres_num = len(genres_dict)\n",
    "    genres_features = [0] * genres_num\n",
    "    genres_list = [genre for genre in genres_str.split('|') if genre != \"(no genres listed)\"]\n",
    "    for genre in genres_list:\n",
    "        genres_features[genres_dict[genre]] = 1\n",
    "    return(genres_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_popular_tags = df_tags\\\n",
    "    .groupby('tag')\\\n",
    "    .agg({'userId': 'count'}).reset_index()\\\n",
    "    .sort_values(by=['userId'], ascending=False)\\\n",
    "    .head(150)\n",
    "# df_popular_tags.userId.sum() / df_tags.shape[0]\n",
    "popular_tags = df_popular_tags['tag'].values\n",
    "pop_tags_dict = {popular_tags[i]: i for i in range(0,len(popular_tags))}\n",
    "\n",
    "def get_pop_tags_features(tags, pop_tags_dict):\n",
    "    pop_tags = [tag for tag in tags if tag in pop_tags_dict.keys()]\n",
    "    tags_num = len(pop_tags_dict)\n",
    "    pop_tags_features = [0] * tags_num\n",
    "    if (len(pop_tags) > 0):\n",
    "        weight = 1 / len(tags)\n",
    "        for tag in pop_tags:\n",
    "            pop_tags_features[pop_tags_dict[tag]] = weight\n",
    "    return(pop_tags_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevant_tags = df_genome\\\n",
    "    .groupby('tag')\\\n",
    "    .agg({'relevance': 'mean'}).reset_index()\\\n",
    "    .sort_values(by=['relevance'], ascending=False)\\\n",
    "    .head(150)\n",
    "relevant_tags = df_relevant_tags['tag'].values\n",
    "rel_tags_dict = {relevant_tags[i]: i for i in range(0,len(relevant_tags))}\n",
    "\n",
    "def get_rel_tags_features(genomes, rel_tags_dict):\n",
    "    try:\n",
    "        rel_genome = [genome for genome in genomes if genome[0] in rel_tags_dict.keys()]\n",
    "    except:\n",
    "        rel_genome = []\n",
    "    tags_num = len(rel_tags_dict)\n",
    "    rel_tags_features = [0] * tags_num\n",
    "    if (len(rel_genome) > 0):\n",
    "        for [tag, relevance] in rel_genome:\n",
    "            rel_tags_features[rel_tags_dict[tag]] = relevance\n",
    "    return(rel_tags_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>genres</th>\n",
       "      <th>tag</th>\n",
       "      <th>genome</th>\n",
       "      <th>genres_features</th>\n",
       "      <th>tag_features</th>\n",
       "      <th>pop_tag_features</th>\n",
       "      <th>rel_tag_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9000</td>\n",
       "      <td>318</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1167308733</td>\n",
       "      <td>Crime|Drama</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[007, 0.02250000000000002], [007 (series), 0....</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.889, 0.8892500000000001, 0.561, 0.9515, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9000</td>\n",
       "      <td>337</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1167308940</td>\n",
       "      <td>Drama</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[007, 0.01924999999999999], [007 (series), 0....</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.8055000000000001, 0.61575, 0.46375, 0.42475...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9000</td>\n",
       "      <td>508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1167308922</td>\n",
       "      <td>Drama</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[007, 0.024749999999999998], [007 (series), 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.7537499999999999, 0.7809999999999999, 0.600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9000</td>\n",
       "      <td>1193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1167308808</td>\n",
       "      <td>Drama</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[007, 0.020000000000000014], [007 (series), 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.8812500000000001, 0.82375, 0.485, 0.8247500...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>1207</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1167308966</td>\n",
       "      <td>Drama</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[[007, 0.019000000000000017], [007 (series), 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.76675, 0.8267500000000001, 0.47825, 0.65075...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp       genres    tag  \\\n",
       "0    9000      318     1.0  1167308733  Crime|Drama  [nan]   \n",
       "1    9000      337     1.0  1167308940        Drama  [nan]   \n",
       "2    9000      508     1.0  1167308922        Drama  [nan]   \n",
       "3    9000     1193     1.0  1167308808        Drama  [nan]   \n",
       "4    9000     1207     1.0  1167308966        Drama  [nan]   \n",
       "\n",
       "                                              genome  \\\n",
       "0  [[007, 0.02250000000000002], [007 (series), 0....   \n",
       "1  [[007, 0.01924999999999999], [007 (series), 0....   \n",
       "2  [[007, 0.024749999999999998], [007 (series), 0...   \n",
       "3  [[007, 0.020000000000000014], [007 (series), 0...   \n",
       "4  [[007, 0.019000000000000017], [007 (series), 0...   \n",
       "\n",
       "                                     genres_features  \\\n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        tag_features  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    pop_tag_features  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    rel_tag_features  \n",
       "0  [0.889, 0.8892500000000001, 0.561, 0.9515, 0.9...  \n",
       "1  [0.8055000000000001, 0.61575, 0.46375, 0.42475...  \n",
       "2  [0.7537499999999999, 0.7809999999999999, 0.600...  \n",
       "3  [0.8812500000000001, 0.82375, 0.485, 0.8247500...  \n",
       "4  [0.76675, 0.8267500000000001, 0.47825, 0.65075...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rmtg['genres_features'] = df_rmtg['genres'].map(lambda x: get_genres_features(x, genres_dict))\n",
    "df_rmtg['pop_tag_features'] = df_rmtg['tag'].map(lambda x: get_pop_tags_features(x, pop_tags_dict))\n",
    "df_rmtg['rel_tag_features'] = df_rmtg['genome'].map(lambda x: get_rel_tags_features(x, rel_tags_dict))\n",
    "df_rmtg.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmtg['features1'] = df_rmtg['genres_features']\n",
    "df_rmtg['features2'] = df_rmtg.\\\n",
    "    apply(lambda x: x['genres_features'] + x['rel_tag_features'], axis=1)\n",
    "df_rmtg['features3'] = df_rmtg.\\\n",
    "    apply(lambda x: x['genres_features'] + x['pop_tag_features'] + x['rel_tag_features'], axis=1)\n",
    "df_abt = df_rmtg[['userId', 'movieId', 'rating', 'features1', 'features2', 'features3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>features1</th>\n",
       "      <th>features2</th>\n",
       "      <th>features3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9000</td>\n",
       "      <td>318</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9000</td>\n",
       "      <td>337</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9000</td>\n",
       "      <td>508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9000</td>\n",
       "      <td>1193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>1207</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating                                          features1  \\\n",
       "0    9000      318     1.0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1    9000      337     1.0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2    9000      508     1.0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3    9000     1193     1.0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    9000     1207     1.0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           features2  \\\n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           features3  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abt.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous experience we know that people can change their preferences and opinions over time. Therefore, to ensure the quality of our data, we will use only ratings given since 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(ts):\n",
    "    return datetime.fromtimestamp(int(ts)).year\n",
    "\n",
    "df_ratings['year'] = df_ratings['timestamp'].apply(get_year)\n",
    "min_year = 2015\n",
    "df_ratings_since = df_ratings[df_ratings.year > min_year][['userId', 'movieId', 'rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's confirm or deny the _WAP_'s hypothesis: Are the six proposed movies truly unpopular, but good?\n",
    "\n",
    "To do that, let's count the number of total opinions, and the average rating of all the movies in the dataset, and then compare them with the _WAP_ movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = df_ratings_since.groupby(['movieId']).agg({'rating':['count', np.mean]}).rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wap_movies = np.array([2395, 4979, 30810, 55269, 72226, 94959])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_movies['count']\n",
    "y = df_movies['mean']\n",
    "x_wap = df_movies[df_movies.index.isin(wap_movies)]['count']\n",
    "y_wap = df_movies[df_movies.index.isin(wap_movies)]['mean']\n",
    "\n",
    "plt.scatter(x, y, c=\"green\", alpha=0.5, s=15)\n",
    "plt.scatter(x_wap, y_wap, c=\"red\", s=15)\n",
    "plt.xlabel(\"freq\")\n",
    "plt.ylabel(\"avg rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watching the red points (_WAP_ movies), they are up to the left. In other words, the six proposed movies seem to be unpopular, but good, confirming the hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAP ratings dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first idea for the model dataset is to use as items only the six _WAP_ proposed movies. Since now, we will call this, the `wap_ratings` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wap_ratings = df_ratings_since[df_ratings_since.movieId.isin(wap_movies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target ratings dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, working with only six items could be insufficient for a recommendation system. Then, we can add other movies with the same unpopular-but-good situation, build a more general recommendation system, and test if it improves our results. Since now, we will call this kind of films, _target movies_.\n",
    "\n",
    "To be more specific, we will define a _target movie_ as one with between 1500 and 4000 opinions since 2015, and an average rating greater or equal than 3.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "plt.scatter(x, y, c=\"green\", alpha=0.5, s=10)\n",
    "plt.scatter(x_wap, y_wap, c=\"red\", s=10)\n",
    "ax = plt.gca()\n",
    "ax.add_patch(Rectangle((1500, 3.6), 2500, 0.8, color='b', alpha=0.3))\n",
    "plt.xlabel(\"freq\")\n",
    "plt.ylabel(\"avg rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, the idea for _Phase 1_ is to test different approaches. Then, we want to keep the development dataset compact to reduce computational complexity. Because of that, we will use 40 random _target_ movies for the `target_ratings` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_movies_filter = (df_movies['count'] >= 1500) & (df_movies['count'] <= 4000) &\\\n",
    "    (df_movies['mean'] >= 3.6) & (~df_movies.index.isin(wap_movies))\n",
    "similar_movies = df_movies[similar_movies_filter].index\n",
    "m_sample_size = 40\n",
    "similar_movies_sampled = np.random.choice(similar_movies, m_sample_size)\n",
    "target_movies = np.concatenate((wap_movies, similar_movies_sampled))\n",
    "df_target_ratings = df_ratings_since[df_ratings_since.movieId.isin(target_movies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed ratings dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have two datasets: `wap_ratings`, with the six _WAP_ proposed films, and `target_ratings`, that contains all the movies from the former, adding more random unpopular-but-good movies. But, what if we are missing relevant information from other kinds of films? \n",
    "\n",
    "Let's build a third dataset: `mixed_ratings`. It will add to `target_ratings` films at least as popular as the previous ones (more than 1500 opinions since 2015), but which are not considered _target_, maybe because they have more than 4000 opinions, or their average rating is lower than 3.6.  \n",
    "\n",
    "Again, to keep the dataset simple, we will add only 50 films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_target_movies = np.setdiff1d(df_movies[df_movies['count'] >= 1500].index, target_movies)\n",
    "m_sample_size = 50\n",
    "not_target_movies_sampled = np.random.choice(not_target_movies, m_sample_size)\n",
    "mixed_movies = np.concatenate((target_movies, not_target_movies_sampled))\n",
    "df_mixed_ratings = df_ratings_since[df_ratings_since.movieId.isin(mixed_movies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, the idea for this phase is to test several different approaches and compare them, to know which one we can go deeper into. Then, to have a fair comparison, we will work with the same users for the three datasets. Since the `wap_ratings` is a subset of the other two, we are going to select users who have rated at least one of the _WAP_ films, assuming the cost of the bias in which we are incurring. Therefore, in the three datasets, all the users will have rated at least two of the WAP films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wap_ratings = df_ratings_since[df_ratings_since.movieId.isin(wap_movies)]\n",
    "counts = df_wap_ratings['userId'].value_counts()\n",
    "user_subset = df_wap_ratings[df_wap_ratings.userId.isin(counts.index[counts > 1])].userId.unique().tolist()\n",
    "df_wap_ratings = df_wap_ratings[df_wap_ratings.userId.isin(user_subset)]\n",
    "df_target_ratings = df_target_ratings[df_target_ratings.userId.isin(user_subset)]\n",
    "df_mixed_ratings = df_mixed_ratings[df_mixed_ratings.userId.isin(user_subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of ratings (WAP): \" + str(len(df_wap_ratings)))\n",
    "print(\"Number of ratings (Target): \" + str(len(df_target_ratings)))\n",
    "print(\"Number of ratings (Mixed): \" + str(len(df_mixed_ratings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Describe split train_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_set = df_wap_ratings.sample(frac = 1.0).groupby('userId').head(1).sample(frac = 0.40)\n",
    "df_wap_ratings_train = pd.concat([df_wap_ratings, df_test_set]).drop_duplicates(keep=False)\n",
    "df_target_ratings_train = pd.concat([df_target_ratings, df_test_set]).drop_duplicates(keep=False)\n",
    "df_mixed_ratings_train = pd.concat([df_mixed_ratings, df_test_set]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surprise Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Brief description of the library's objectives, specs, developers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset, model_selection\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "def create_surprise_ratings(df_ratings):\n",
    "    reader = Reader(rating_scale=(0.5, 5.0))\n",
    "    sp_ratings = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
    "    return sp_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_wap_ratings_train = create_surprise_ratings(df_wap_ratings_train)\n",
    "sp_target_ratings_train = create_surprise_ratings(df_target_ratings_train)\n",
    "sp_mixed_ratings_train = create_surprise_ratings(df_mixed_ratings_train)\n",
    "sp_test_set = create_surprise_ratings(df_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraction of Concordant Pairs (FCP) is an error measure proposed in Collaborative Filtering on Ordinal User Feedback by Koren and Sill, and which is already implemented in surprise.\n",
    "\n",
    "Given a test set, the number of concordant pairs ($n_{c}^{u}$) for user $u$ is defined as the amount of pairs ranked correctly by rating predictor $\\hat{r}_u$. In mathematical terms,\n",
    "\n",
    "$$n_{c}^{u} = |\\{(i, j) | \\hat{r}_{ui} > \\hat{r}_{uj} \\text{ and } r_{ui} > r_{uj}\\}|.$$\n",
    "On the other side, the number of discordant pairs are the ones which were misranked:\n",
    "\n",
    "$$n_{d}^{u} = |\\{(i, j) | \\hat{r}_{ui} \\leq \\hat{r}_{uj} \\text{ and } r_{ui} > r_{uj}\\}|.$$\n",
    "To get the total of concordant and discordant pairs, we just sum among every user.\n",
    "\n",
    "$$n_c = \\sum_{u}n_{c}^{u}$$\n",
    "$$n_d = \\sum_{u}n_{d}^{u}$$\n",
    "\n",
    "Finally, the $FCP$ error measure is defined as $$FCP = \\frac{n_c}{n_c + n_d},$$\n",
    "\n",
    "where 1.0 means perfect order, and 0.0 means completely wrong order. Then, the higher is $FCP$, the better is the model.\n",
    "\n",
    "This measure is useful because it tests the accurate order of the values, more than the specific scale. As our problem is an ordinal one, we are going to take this as our __primary accuracy metric__.\n",
    "\n",
    "Also, we will use the classic RMSE as our __secondary accuracy metric__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will be determining the best hyperparameters and models to use in our analysis. Firstly, we will use GridSearchCV to find the best hyperparameters (this can be found within the `surprise` model_selection package. After that is complete, we will compare the models to determine the best from each of the baseline, neighborhood and model based techniques. Within neighborhood we tested basic, mean and zscore knn implementations and for model based, we used svd and svd++. Finally, we will use our held out testing set to compare the baseline, neighbourhood and model based approaches, selecting whichever model has the highest relative accuracy (using FCP).\n",
    "\n",
    "We will also be testing the above process using three different datasets. The `WAP`, `Target` and `Mixed` sets outlined earlier in the report. The goal of this is to determine if adding more information about other films results in signficant performance enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import BaselineOnly, KNNBasic, KNNWithMeans, KNNWithZScore, SVD, SVDpp\n",
    "cv = 5\n",
    "measures = ['fcp', 'rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_grid_models(sp_ratings, cv, measures):\n",
    "    np.random.seed(2018)\n",
    "    models = {}\n",
    "    models['baseline'] = fit_grid_Baseline(sp_ratings, cv, measures)\n",
    "    models['knn_basic'] = fit_grid_KNN(sp_ratings, cv, measures, 'basic')\n",
    "    models['knn_means'] = fit_grid_KNN(sp_ratings, cv, measures, 'means')\n",
    "    models['knn_zscore'] = fit_grid_KNN(sp_ratings, cv, measures, 'zscore')\n",
    "    models['svd'] = fit_grid_SVD(sp_ratings, cv, measures, 'svd')\n",
    "    models['svdpp'] = fit_grid_SVD(sp_ratings, cv, measures, 'svdpp')\n",
    "    return(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_grid_Baseline(sp_ratings, cv, measures, reg_u=[0], reg_i=[0]):\n",
    "    param_grid = {'bsl_options': {'reg_u': reg_u, 'reg_i': reg_i}}\n",
    "    gs_Baseline = model_selection.GridSearchCV(BaselineOnly, param_grid, measures=['fcp', 'rmse'], cv=cv)\n",
    "    gs_Baseline.fit(sp_ratings)\n",
    "    return(gs_Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_grid_KNN(\n",
    "    sp_ratings, cv, measures, knn,\n",
    "    user_based=[False], \n",
    "    k=[5, 10, 15],\n",
    "    reg=[0.0001, 0.001, 0.01], \n",
    "    sim_name=['pearson', 'pearson_baseline', 'cosine'],\n",
    "    sim_min_support=[2, 4],\n",
    "    method=['als']\n",
    "):\n",
    "    param_grid = {'bsl_options': {'method': method,\n",
    "                              'reg': reg},\n",
    "              'k': k,\n",
    "              'sim_options': {'name': sim_name,\n",
    "                              'min_support': sim_min_support,\n",
    "                              'user_based': user_based}\n",
    "              }\n",
    "    knn_methods = {\n",
    "        'basic': KNNBasic,\n",
    "        'means': KNNWithMeans,\n",
    "        'zscore': KNNWithZScore\n",
    "    }\n",
    "    gs_KNN = model_selection.GridSearchCV(knn_methods[knn], param_grid, measures=measures, cv=cv)\n",
    "    gs_KNN.fit(sp_ratings)\n",
    "    return(gs_KNN)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_grid_SVD(\n",
    "    sp_ratings, cv, measures, svd,\n",
    "    n_factors=[5, 10, 20],\n",
    "    lr_all=[0.001, 0.01, 0.1],\n",
    "    reg_all=[0.001, 0.01, 0.1]\n",
    "):\n",
    "    param_grid = {'n_factors': n_factors, 'lr_all': lr_all, 'reg_all': reg_all}\n",
    "    svd_methods = {\n",
    "        'svd': SVD,\n",
    "        'svdpp': SVDpp\n",
    "    }\n",
    "    gs_SVD = model_selection.GridSearchCV(svd_methods[svd], param_grid, measures=measures, cv=cv)\n",
    "    gs_SVD.fit(sp_ratings)\n",
    "    return(gs_SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_grid_models_accuracy(models):\n",
    "    print('FCP metric')\n",
    "    for model in models.keys():\n",
    "        print(str(model) + \" accuracy: \" + str(round(models[model].best_score['fcp'], 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Models Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we test the accuracy of each model on each dataset after tuning the hyperparameters. This stage is crucial as a preliminary stage between creating a singular baseline model and determining the final model type, parameters, and data subset to use. By testing small chunks of data we can speed up this process and focus the bulk of our time on only the top models and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "wap_models = fit_grid_models(sp_wap_ratings_train, cv, measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grid_models_accuracy(wap_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "target_models = fit_grid_models(sp_target_ratings_train, cv, measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grid_models_accuracy(target_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "mixed_models = fit_grid_models(sp_mixed_ratings_train, cv, measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grid_models_accuracy(mixed_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "from collections import defaultdict\n",
    "from six import iteritems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the correspondent hyperparameters, we will take the neighborhood-based and model-based techniques with the best scores for each `train dataset`, and test them with the real `test set`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the hyperparameters section, where for simplicity we just used the `surprise` implemented methods, in this final model selection we are going to make some changes so that we are able to conduct a fair comparison between the datasets.\n",
    "\n",
    "Firstly, instead of doing cross-validation, we are going to use a dataset `test_set` which is entirely different from any of the data within the training set.\n",
    "\n",
    "Secondly, we are not going to calculate the accuracy metric for the entire dataset. Conversely, even when we train the model with several films, we will figure the FCP just taking into account the WAP movies, as this is the core business function of our new recommendation system.\n",
    "\n",
    "Finally, we were suspicious about the FCP values we were getting from the `surprise` method. After inspecting the original code, we realized it is not implemented exactly as the paper defined it. We believe the implemented method is useful to choose the hyperparameters because the main difference is located within the handling of scale. However, for this section, we decided to code ours with some modifications so that it can be interpreted as the one proposed on the paper and that we described previously in this document.\n",
    "\n",
    "To achieve this changes, we propose the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcp_wap(predictions_train, predictions_test):\n",
    "    \"\"\"Modified FCP function for the WAP movies, and implementing the paper's definition.\n",
    "    Args:\n",
    "        predictions_train (:obj:`list` of :obj:`Prediction\\\n",
    "            <surprise.prediction_algorithms.predictions.Prediction>`):\n",
    "            A list of predictions, as returned by the :meth:`test()\n",
    "            <surprise.prediction_algorithms.algo_base.AlgoBase.test>` method.\n",
    "        predictions_test (:obj:`list` of :obj:`Prediction\\\n",
    "            <surprise.prediction_algorithms.predictions.Prediction>`):\n",
    "            A list of predictions, as returned by the :meth:`test()\n",
    "            <surprise.prediction_algorithms.algo_base.AlgoBase.test>` method.\n",
    "        verbose: If True, will print computed value. Default is ``True``.\n",
    "    Returns:\n",
    "        The Fraction of Concordant Pairs.\n",
    "    Raises:\n",
    "        ValueError: When ``predictions`` is empty.\n",
    "    \"\"\"\n",
    "\n",
    "    if not predictions_test:\n",
    "        raise ValueError('Prediction list is empty.')\n",
    "\n",
    "    predictions_u = defaultdict(list)\n",
    "    nc_u = defaultdict(int)\n",
    "    nd_u = defaultdict(int)\n",
    "    fcp_u = defaultdict(float)\n",
    "    wap_movies = np.array([2395, 4979, 30810, 55269, 72226, 94959])\n",
    "\n",
    "    for u0, m, r0, est, _ in predictions_train:\n",
    "        if m in wap_movies:\n",
    "            predictions_u[u0].append((r0, r0, 'train'))\n",
    "    \n",
    "    for u0, m, r0, est, _ in predictions_test:\n",
    "        if m in wap_movies:\n",
    "            predictions_u[u0].append((r0, est, 'test'))\n",
    "\n",
    "    for u0, preds in iteritems(predictions_u):\n",
    "        for r0i, esti, kindi in preds:\n",
    "            if kindi == 'test':\n",
    "                for r0j, estj, kindj in preds:\n",
    "                    if ((esti > estj and r0i > r0j) or (esti == estj and r0i == r0j)):\n",
    "                        nc_u[u0] += 1\n",
    "                    if esti >= estj and r0i < r0j:\n",
    "                        nd_u[u0] += 1\n",
    "        try:\n",
    "            fcp_u[u0] = nc_u[u0] / (nc_u[u0] + nd_u[u0])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    fcp = round(np.mean(list(fcp_u.values())), 4)\n",
    "\n",
    "    return fcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_accuracy(sp_test, sp_train, models, technique):\n",
    "    algo = models[technique].best_estimator['fcp']\n",
    "    trainset = sp_train.build_full_trainset()\n",
    "    testset = sp_test.build_full_trainset()\n",
    "    algo.fit(trainset)\n",
    "    predictions_train = algo.test(trainset.build_testset())\n",
    "    predictions_test = algo.test(testset.build_testset())\n",
    "    accuracy = fcp_wap(predictions_train, predictions_test)\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the three datasets are run through our custom FCP accuracy algorithm using the tuned baseline, knn and svd++ algorithms. Please note that the fcp accuracies have been customized for this given project and should only be used for comparison purposes. They do not represent the overall accuracy in predicting all movies over the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "wap_baseline_fcp = get_test_accuracy(sp_test_set, sp_wap_ratings_train, wap_models, 'baseline')\n",
    "wap_knn_fcp = get_test_accuracy(sp_test_set, sp_wap_ratings_train, wap_models, 'knn_means')\n",
    "wap_svd_fcp = get_test_accuracy(sp_test_set, sp_wap_ratings_train, wap_models, 'svdpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('baseline test accuracy: ' + str(wap_baseline_fcp))\n",
    "print('knn_zscore test accuracy: ' + str(wap_knn_fcp))\n",
    "print('svdpp accuracy: ' + str(wap_svd_fcp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "target_baseline_fcp = get_test_accuracy(sp_test_set, sp_target_ratings_train, target_models, 'baseline')\n",
    "target_knn_fcp = get_test_accuracy(sp_test_set, sp_target_ratings_train, target_models, 'knn_means')\n",
    "target_svd_fcp = get_test_accuracy(sp_test_set, sp_target_ratings_train, target_models, 'svdpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('baseline test accuracy: ' + str(target_baseline_fcp))\n",
    "print('knn_means test accuracy: ' + str(target_knn_fcp))\n",
    "print('svdpp accuracy: ' + str(target_svd_fcp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "mixed_baseline_fcp = get_test_accuracy(sp_test_set, sp_mixed_ratings_train, mixed_models, 'baseline')\n",
    "mixed_knn_fcp = get_test_accuracy(sp_test_set, sp_mixed_ratings_train, mixed_models, 'knn_zscore')\n",
    "mixed_svd_fcp = get_test_accuracy(sp_test_set, sp_mixed_ratings_train, mixed_models, 'svdpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('baseline test accuracy: ' + str(mixed_baseline_fcp))\n",
    "print('knn_means test accuracy: ' + str(mixed_knn_fcp))\n",
    "print('svdpp accuracy: ' + str(mixed_svd_fcp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when there was a bias by construction, which we expect helped the `wap ratings` dataset (we selected only users with 2 or more _WAP_ movies ratings, the dataset with the best performance was the `mixed ratings` one. Particularly, we got the best results when we used the svd++ technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Film Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows from here is to find which is the _WAP_ film with the greatest estimated rating for each user, and send her/him the recommendation. Also, it can be unrealistic to send a recommendation for everybody, because there will be users for which all the _WAP_ movies have low ratings. Then, for further phases of this project, we must set a threshold. Maybe a direct scale rating one, or maybe a minimum quantile among all the estimated movies per customer so that we take into account the scale differences among users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing `Phase 1` of our project our team has reached the conclusion that using a larger subset of movies (even those not related to WAP's movies) would be extremely beneficial to the overall accuracy of our model. Also, we have found that the SVD++ model consistently has the highest FCP accuracy of WAP movies as the dataset gains movies. Though our model is still in its infancy the results we have been able to obtain show promise in the potential success that we can obtain by targetting advertisments on behalf of Wes Anderson Productions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the time contraints associated with training and testing our models, we were unable to complete certain visualizations that we believe are important to show the client. Therefore, the following will be included in `Phase 2` of our project, when we present our findings with Wes Anderson Productions.\n",
    "* Create plots of how the error measure changes when we fix all the hyperparameters, except one.\n",
    "* Get the time it takes to fit each model (on a standard performance VM [ex: aws ec2 t3.large]).\n",
    "* Since we decided that we will use the complete dataset, we know can recommend _WAP_ movies, even if the user hasn't seen any of them. Then, we can use datasets that reflect this advantage, and we must investigate how this will change the _WARS_ performance.\n",
    "* Try with recommendation techniques outside the `surprise` package, like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
